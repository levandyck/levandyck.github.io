<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Contributions | Lenny van Dyck </title> <meta name="author" content="Leonard E. van Dyck"> <meta name="description" content=""> <meta name="keywords" content="levandyck, Leonard van Dyck, Lenny van Dyck, van Dyck, science, neuroscience, psychology, computer science, brain, neuroimaging"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/brain-solid.svg?e0c214f6ae8b59c6194009cb331d2df3"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://levandyck.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?2ff405c388088e95d2c53028e2c8ad8c"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Lenny van Dyck </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Contributions <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/CV_vanDyck.pdf" target="_blank" rel="noopener noreferrer">CV <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Contributions</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <a id="Peer-reviewed manuscripts"></a> <p class="bibtitle" style="font-size: 1.5em;">Peer-reviewed manuscripts</p> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sttt24-480.webp 480w,/assets/img/publication_preview/sttt24-800.webp 800w,/assets/img/publication_preview/sttt24-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sttt24.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sttt24.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_bremmer_dobs_2024" class="col-sm-8"> <div class="title">Artificial intelligence meets body sense: Task-driven neural networks reveal computational principles of the proprioceptive pathway</div> <div class="author"> van Dyck, L. E., <a href="https://www.uni-marburg.de/en/fb13/neurophysics/team/prof-dr-frank-bremmer/" rel="external nofollow noopener" target="_blank">Bremmer, F.</a>, and <a href="https://www.katharinadobs.com/" rel="external nofollow noopener" target="_blank">Dobs, K.</a> </div> <div class="periodical"> <em>Signal Transduction and Targeted Therapy</em> </div> <div class="periodical"> [Open Access] </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s41392-024-01870-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In a recent study published in Cell, Marin Vargas and Bisi et al. present an innovative approach to unravel the computational principles underlying proprioceptive processing in non-human primates. Their findings showcase the utility of task-driven modeling in advancing neuroscience and offer translational potential by providing seminal insights into the goals and mechanisms by which the brain encodes body position and movements.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jocn22-480.webp 480w,/assets/img/publication_preview/jocn22-800.webp 800w,/assets/img/publication_preview/jocn22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/jocn22.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jocn22.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_gruber_2023" class="col-sm-8"> <div class="title">Modeling biological face recognition with deep convolutional neural networks</div> <div class="author"> van Dyck, L. E., and <a href="https://www.researchgate.net/profile/Walter-Gruber/" rel="external nofollow noopener" target="_blank">Gruber, W. R.</a> </div> <div class="periodical"> <em>Journal of Cognitive Neuroscience</em> </div> <div class="periodical"> [Open Access] </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1162/jocn_a_02040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Deep convolutional neural networks (DCNNs) have become the state-of-the-art computational models of biological object recognition. Their remarkable success has helped vision science break new ground, and recent efforts have started to transfer this achievement to research on biological face recognition. In this regard, face detection can be investigated by comparing face-selective biological neurons and brain areas to artificial neurons and model layers. Similarly, face identification can be examined by comparing in vivo and in silico multidimensional “face spaces.” In this review, we summarize the first studies that use DCNNs to model biological face recognition. On the basis of a broad spectrum of behavioral and computational evidence, we conclude that DCNNs are useful models that closely resemble the general hierarchical organization of face recognition in the ventral visual pathway and the core face network. In two exemplary spotlights, we emphasize the unique scientific contributions of these models. First, studies on face detection in DCNNs indicate that elementary face selectivity emerges automatically through feedforward processing even in the absence of visual experience. Second, studies on face identification in DCNNs suggest that identity-specific experience and generative mechanisms facilitate this particular challenge. Taken together, as this novel modeling approach enables close control of predisposition (i.e., architecture) and experience (i.e., training data), it may be suited to inform long-standing debates on the substrates of biological face recognition.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/frontneurosci22-480.webp 480w,/assets/img/publication_preview/frontneurosci22-800.webp 800w,/assets/img/publication_preview/frontneurosci22-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/frontneurosci22.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="frontneurosci22.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_denzler_gruber_2022" class="col-sm-8"> <div class="title">Guiding visual attention in deep convolutional neural networks based on human eye movements</div> <div class="author"> van Dyck, L. E., Denzler, S. J., and <a href="https://www.researchgate.net/profile/Walter-Gruber/" rel="external nofollow noopener" target="_blank">Gruber, W. R.</a> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em> </div> <div class="periodical"> [Open Access] </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/fnins.2022.975639" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.3389/fnins.2022.975639"></span> </div> <div class="abstract hidden"> <p>Deep Convolutional Neural Networks (DCNNs) were originally inspired by principles of biological vision, have evolved into best current computational models of object recognition, and consequently indicate strong architectural and functional parallelism with the ventral visual pathway throughout comparisons with neuroimaging and neural time series data. As recent advances in deep learning seem to decrease this similarity, computational neuroscience is challenged to reverse-engineer the biological plausibility to obtain useful models. While previous studies have shown that biologically inspired architectures are able to amplify the human-likeness of the models, in this study, we investigate a purely data-driven approach. We use human eye tracking data to directly modify training examples and thereby guide the models’ visual attention during object recognition in natural images either toward or away from the focus of human fixations. We compare and validate different manipulation types (i.e., standard, human-like, and non-human-like attention) through GradCAM saliency maps against human participant eye tracking data. Our results demonstrate that the proposed guided focus manipulation works as intended in the negative direction and non-human-like models focus on significantly dissimilar image parts compared to humans. The observed effects were highly category-specific, enhanced by animacy and face presence, developed only after feedforward processing was completed, and indicated a strong influence on face detection. With this approach, however, no significantly increased human-likeness was found. Possible applications of overt visual attention in DCNNs and further implications for theories of face detection are discussed.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/frontneurosci21-480.webp 480w,/assets/img/publication_preview/frontneurosci21-800.webp 800w,/assets/img/publication_preview/frontneurosci21-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/frontneurosci21.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="frontneurosci21.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_kwitt_denzler_gruber_2021" class="col-sm-8"> <div class="title">Comparing object recognition in humans and deep convolutional neural networks—an eye tracking study</div> <div class="author"> van Dyck, L. E., <a href="https://rkwitt.github.io/" rel="external nofollow noopener" target="_blank">Kwitt, R.</a>, Denzler, S. J., and <a href="https://www.researchgate.net/profile/Walter-Gruber/" rel="external nofollow noopener" target="_blank">Gruber, W. R.</a> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em> </div> <div class="periodical"> [Open Access] </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/fnins.2021.750639" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.3389/fnins.2021.750639"></span> </div> <div class="abstract hidden"> <p>Deep convolutional neural networks (DCNNs) and the ventral visual pathway share vast architectural and functional similarities in visual challenges such as object recognition. Recent insights have demonstrated that both hierarchical cascades can be compared in terms of both exerted behavior and underlying activation. However, these approaches ignore key differences in spatial priorities of information processing. In this proof-of-concept study, we demonstrate a comparison of human observers (N = 45) and three feedforward DCNNs through eye tracking and saliency maps. The results reveal fundamentally different resolutions in both visualization methods that need to be considered for an insightful comparison. Moreover, we provide evidence that a DCNN with biologically plausible receptive field sizes called vNet reveals higher agreement with human viewing behavior as contrasted with a standard ResNet architecture. We find that image-specific factors such as category, animacy, arousal, and valence have a direct link to the agreement of spatial object recognition priorities in humans and DCNNs, while other measures such as difficulty and general image properties do not. With this approach, we try to open up new perspectives at the intersection of biological and computer vision research.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/-480.webp 480w,/assets/img/publication_preview/-800.webp 800w,/assets/img/publication_preview/-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/" class="preview z-depth-1 rounded" width="100%" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hertlein_vandyck_2020" class="col-sm-8"> <div class="title">Predicting engagement in electronic surveillance in romantic relationships</div> <div class="author"> Hertlein, K. M., and van Dyck, L. E.</div> <div class="periodical"> <em>Cyberpsychology, Behavior, and Social Networking</em> </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1089/cyber.2019.0424" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <a id="Conference proceedings"></a> <p class="bibtitle" style="font-size: 1.5em;">Conference proceedings</p> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="vandyck_hebart_dobs_2024b" class="col-sm-8"> <div class="title">Core neural dimensions of functionally selective areas in the human visual cortex</div> <div class="author"> van Dyck, L. E., <a href="http://martin-hebart.de/" rel="external nofollow noopener" target="_blank">Hebart, M. N.</a>, and <a href="https://www.katharinadobs.com/" rel="external nofollow noopener" target="_blank">Dobs, K.</a> </div> <div class="periodical"> <em>In Cognitive Computational Neuroscience (CCN)</em><br><span>Boston, MA, USA</span> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://2024.ccneuro.org/pdf/124_Paper_authored_ManuscriptAuthored.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Prior research has extensively documented functional selectivity for categories within visual cortical areas, primarily by contrasting neural responses to images from various categories. However, such categorical approaches are less suitable to capture the diversity of neural representations within these areas. Do category-selective areas encode holistic categories, or are they instead tuned to multifaceted features? To address this question, we employed non-negative matrix factorization (NMF) for analyzing human fMRI responses to natural images in face-, body-, and scene-selective areas, which uncovered a consistent set of interpretable neural dimensions across participants. These dimensions not only aligned with the areas’ respective category preferences, but also revealed finer within-category distinctions, indicating selective tuning to diverse visual input features. Mapping these dimensions onto the cortical surface showed both clustered and distributed topographies, which accounted for overlaps between areas. Our results suggest that category-selective areas show multifaceted feature tuning, challenging traditional views and highlighting the complex interplay of neural dimensions in encoding visual information.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="vandyck_hebart_dobs_2024a" class="col-sm-8"> <div class="title">Neural representational dimensions capture the nested functional organization of the human visual cortex</div> <div class="author"> van Dyck, L. E., <a href="http://martin-hebart.de/" rel="external nofollow noopener" target="_blank">Hebart, M. N.</a>, and <a href="https://www.katharinadobs.com/" rel="external nofollow noopener" target="_blank">Dobs, K.</a> </div> <div class="periodical"> <em>In SFB Workshop Categorization in Perception and Action: Minds, Models, Mechanisms</em><br><span>Marburg, Germany</span> </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="vandyck_dobs_hebart_2024" class="col-sm-8"> <div class="title">Core neural dimensions of functionally selective areas in the human visual cortex</div> <div class="author"> van Dyck, L. E., <a href="https://www.katharinadobs.com/" rel="external nofollow noopener" target="_blank">Dobs, K.</a>, and <a href="http://martin-hebart.de/" rel="external nofollow noopener" target="_blank">Hebart, M. N.</a> </div> <div class="periodical"> <em>In Workshop on CONCEPTS, ACTIONS, and OBJECTS (CAOs)</em><br><span>Rovereto, Italy</span> </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="vandyck_denzler_gruber_2023" class="col-sm-8"> <div class="title">Analyzing and increasing the similarity of humans and deep convolutional neural networks in object recognition</div> <div class="author"> van Dyck, L. E., Denzler, S. J., and <a href="https://www.researchgate.net/profile/Walter-Gruber/" rel="external nofollow noopener" target="_blank">Gruber, W. R.</a> </div> <div class="periodical"> <em>In European Conference on Visual Perception (ECVP)</em><br><span>Nijmegen, The Netherlands</span> </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="vandyck_denzler_schoellkopf_gruber_2022" class="col-sm-8"> <div class="title">Spatial similarities between human eye movements and deep convolutional neural network saliency maps across time</div> <div class="author"> van Dyck, L. E., Denzler, S. J., Schöllkopf, C. P., and <a href="https://www.researchgate.net/profile/Walter-Gruber/" rel="external nofollow noopener" target="_blank">Gruber, W. R.</a> </div> <div class="periodical"> <em>In Salzburg Mind-Brain Annual Meeting (SAMBA)</em><br><span>Salzburg, Austria</span> </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <a id="Theses"></a> <p class="bibtitle" style="font-size: 1.5em;">Theses</p> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/-480.webp 480w,/assets/img/publication_preview/-800.webp 800w,/assets/img/publication_preview/-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/" class="preview z-depth-1 rounded" width="100%" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_2023" class="col-sm-8"> <div class="title">Unraveling top-down and bottom-up processes in theory of mind with layer fMRI</div> <div class="author"> van Dyck, L. E.</div> <div class="periodical"> <em>University of Salzburg, Department of Psychology</em> </div> <div class="periodical"> [Open Access] </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/MastersThesis_vanDyck.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Theory of Mind (ToM), the ability to reason about the mental states of others, is a crucial social skill of humans. Neuroimaging studies have found that the Temporo-Parietal Junction (TPJ) and an entire network of additional brain regions are specifically activated during mental state reasoning. Two common tasks that are used to investigate ToM are the False Belief (FB) task and the Social Animations (SA) task, which are thought to activate the posterior part (pTPJ) and the anterior part (aTPJ) of TPJ, respectively. While previous research has suggested potential explanations for this functional specialization, the exact mechanisms are not yet fully understood. In this study, high-resolution layer fMRI was used to examine neural activity in cortical layers of pTPJ and aTPJ during the FB and SA tasks. Firstly, the results corroborate the importance of the ToM network and especially TPJ in mental state reasoning. Secondly, a significant interaction between task and region was revealed, which underlines the expected functional specialization of TPJ clusters. Thirdly, the layer profiles of the two tasks indicated feedback-like activity, but when separated by region, pTPJ showed feedback-like activity for the FB task, while aTPJ displayed feedforward-like activity for the SA task. This pattern was further confirmed by a hierarchical cluster analysis. Overall, these findings suggest that the functional specialization, which is even reflected at the level of cortical layers, may enable TPJ to switch between detecting social cues externally and contemplating about them internally.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/-480.webp 480w,/assets/img/publication_preview/-800.webp 800w,/assets/img/publication_preview/-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/" class="preview z-depth-1 rounded" width="100%" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_gruber_2020" class="col-sm-8"> <div class="title">Seeing eye-to-eye? A comparison of object recognition performance in humans and deep convolutional neural networks under image manipulation</div> <div class="author"> van Dyck, L. E.</div> <div class="periodical"> <em>University of Salzburg, Department of Psychology</em> </div> <div class="periodical"> [Open Access] </div> <div class="links"> <a href="https://doi.org/10.48550/arxiv.2007.06294" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <a id="Science communication"></a> <p class="bibtitle" style="font-size: 1.5em;">Science communication</p> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/-480.webp 480w,/assets/img/publication_preview/-800.webp 800w,/assets/img/publication_preview/-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/" class="preview z-depth-1 rounded" width="100%" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vandyck_dobs_2023" class="col-sm-8"> <div class="title">Modellierung der biologischen Gesichtswahrnehmung mit Künstlicher Intelligenz</div> <div class="author"> van Dyck, L. E., and <a href="https://www.katharinadobs.com/" rel="external nofollow noopener" target="_blank">Dobs, K.</a> </div> <a href="https://app.augenspiegel.com/de/profiles/901d314af1eb/editions/28fb46e319fb7aaf3d1a/pages/page/13" target="_blank" rel="external nofollow noopener">DER AUGENSPIEGEL</a> <div class="periodical"> </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Leonard E. van Dyck. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>